{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mp\n",
    "from math import nan\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Assignements\\Assignment 3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.loadtxt(fname = os.path.join(file_path, 'Sample2.txt'), delimiter= '\\n', dtype= str)\n",
    "data_temp = [i.replace('{','').replace('}', '').split(', ') for i in temp]\n",
    "data = [[float(i) for i in j] for j in data_temp]\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.loadtxt(fname = os.path.join(file_path, 'Book1.csv'), delimiter= ',', skiprows = 1)\n",
    "# k = 1\n",
    "# for i in data:\n",
    "#     print(str(k) + ', ' + str(i[0]) + ', ' + str(int(i[1])))\n",
    "#     k += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predicted_class(data, threshold):\n",
    "#     temp =[]\n",
    "#     for i in data:\n",
    "#         if i[1]>threshold:\n",
    "#             temp.append(i + [1.0])\n",
    "#         else:\n",
    "#             temp.append(i + [0.0])\n",
    "#     return temp\n",
    "\n",
    "# data2= predicted_class(data, 0.5)\n",
    "# data2        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['0.639816239', '0.490992931', '0.623814545', ..., '0.595898691',\n",
       "        '0.511057863', '0.63998183'],\n",
       "       ['A', 'B', 'A', ..., 'A', 'A', 'A'],\n",
       "       ['A', 'B', 'A', ..., 'A', 'A', 'A']], dtype='<U32')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predicted_class(data, threshold):\n",
    "    data_t = np.transpose(data)\n",
    "    temp  = []\n",
    "    temp2 = []\n",
    "    temp3 = []\n",
    "    for i in data_t[1]:\n",
    "        if i>threshold:\n",
    "            temp.append('A')\n",
    "        else:\n",
    "            temp.append('B')\n",
    "    for j in data_t[2]:\n",
    "        if j == 1.0:\n",
    "            temp3.append('A')\n",
    "        else:\n",
    "            temp3.append('B')  \n",
    "            \n",
    "    temp2.append(data_t[1])\n",
    "    temp2.append(temp3)\n",
    "    temp2.append(temp)\n",
    "\n",
    "    return np.asarray(temp2)\n",
    "\n",
    "data2= predicted_class(data, 0.5)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The rows are actual and columns are predicted\n",
    "# The first row is positive and second one is of negative class. The same for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5047., 2832.],\n",
       "       [2360., 5519.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluation_metrics(data_t, threshold):\n",
    "    data = predicted_class(data_t, threshold)\n",
    "\n",
    "# CONFUSION Matrix \n",
    "    actual_Classes = data[-2]    \n",
    "    predicted_Classes = data[-1] \n",
    "    class_uniq = np.unique(actual_Classes)\n",
    "#     print(class_uniq)\n",
    "    n = len(class_uniq)\n",
    "    conf = np.zeros((n,n))\n",
    "\n",
    "    for i in range(len(class_uniq)):\n",
    "        for j in range(len(class_uniq)):\n",
    "\n",
    "            conf[i, j] = np.sum((actual_Classes == class_uniq[i]) & (predicted_Classes == class_uniq[j]))\n",
    "    \n",
    "# Evaluation Metrics\n",
    "    EM = {}\n",
    "    EM['accuracy'] = round(sum(np.diagonal(conf))/len(data[0]),3)\n",
    "    EM['prec_positive'] = round(conf[0][0]/sum(conf[:,0]),3)\n",
    "    temp_rec = round(conf[0][0]/sum(conf[0,:]),3)\n",
    "    temp_func = lambda temp: 0 if bool(np.isnan(temp))  else temp\n",
    "    EM['rec_positive'] = temp_func(temp_rec)\n",
    "#     print(EM['rec_positive'])\n",
    "    EM['F1_positive'] = temp_func(round((2*EM['prec_positive']*EM['rec_positive'])/(EM['prec_positive']+EM['rec_positive']),3))\n",
    "    EM['TPR'] = EM['rec_positive']\n",
    "    EM['sensitivity'] = EM['rec_positive']\n",
    "    EM['TNR'] = temp_func(round(conf[1][1]/sum(conf[1,:]),3))\n",
    "    EM['specificity'] = EM['TNR']\n",
    "    EM['FPR'] = temp_func(round(1 - EM['TNR'],3))\n",
    "    return EM, conf\n",
    "\n",
    "\n",
    "EM, conf = evaluation_metrics(data, 0.5)\n",
    "conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.671,\n",
       " 'prec_positive': 0.681,\n",
       " 'rec_positive': 0.641,\n",
       " 'F1_positive': 0.66,\n",
       " 'TPR': 0.641,\n",
       " 'sensitivity': 0.641,\n",
       " 'TNR': 0.7,\n",
       " 'specificity': 0.7,\n",
       " 'FPR': 0.3}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-7c36841006cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mROC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-44-7c36841006cc>\u001b[0m in \u001b[0;36mROC\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mEM_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mEM_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'FPR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEM_temp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'TPR'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-1f146ff72d2c>\u001b[0m in \u001b[0;36mevaluation_metrics\u001b[1;34m(data_t, threshold)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mactual_Classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mpredicted_Classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mclass_uniq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactual_Classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m#     print(class_uniq)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_uniq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[1;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    264\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[1;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[0mFind\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munique\u001b[0m \u001b[0melements\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignoring\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \"\"\"\n\u001b[1;32m--> 303\u001b[1;33m     \u001b[0mar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[0moptional_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def ROC(data, inc):\n",
    "    t = 0\n",
    "    a = []\n",
    "    while t <= 1:\n",
    "        EM_temp = evaluation_metrics(data, t)[0]\n",
    "        b =[EM_temp['FPR'], EM_temp['TPR']]\n",
    "        a.append(b)\n",
    "        t += inc\n",
    "    return a\n",
    "\n",
    "a = np.transpose(ROC(data, 0.01))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.scatter(a[0], a[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Trapeziodal principle to find the area under the roc curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(a):\n",
    "    area = 0\n",
    "    for i in range(len(a[0])-1):\n",
    "#         print(i)\n",
    "        area += abs((a[0][i+1] - a[0][i])*(a[1][i+1] + a[1][i]))/2\n",
    "    return round(area,3)\n",
    "\n",
    "auc(a)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68138248, 0.66087894])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(data2[1], data2[2], average  = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64056352, 0.7004696 ])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_score(data2[1], data2[2], average  = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalya\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:54: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Accuracy 0.671)\n",
      "(Precision 0.681)\n",
      "(Recall 0.641)\n",
      "(F1 0.66)\n",
      "(TPR 0.641)\n",
      "(FPR 0.3)\n",
      "(Specificity 0.7)\n",
      "(Sensitivity 0.641)\n",
      "(AUC 0.738)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as mp\n",
    "\n",
    "def EM_assignment3(file_path, sampletxt, inc):\n",
    "\n",
    "    def predicted_class(data, threshold):\n",
    "        data_t = np.transpose(data)\n",
    "        temp  = []\n",
    "        temp2 = []\n",
    "        temp3 = []\n",
    "        for i in data_t[1]:\n",
    "            if i>threshold:\n",
    "                temp.append('A')\n",
    "            else:\n",
    "                temp.append('B')\n",
    "        for j in data_t[2]:\n",
    "            if j == 1.0:\n",
    "                temp3.append('A')\n",
    "            else:\n",
    "                temp3.append('B')  \n",
    "\n",
    "        temp2.append(data_t[1])\n",
    "        temp2.append(temp3)\n",
    "        temp2.append(temp)\n",
    "\n",
    "        return np.asarray(temp2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def evaluation_metrics(data_t, threshold):\n",
    "        data = predicted_class(data_t, threshold)\n",
    "\n",
    "    # CONFUSION Matrix \n",
    "        actual_Classes = data[-2]    \n",
    "        predicted_Classes = data[-1] \n",
    "        class_uniq = np.unique(actual_Classes)\n",
    "    #     print(class_uniq)\n",
    "        n = len(class_uniq)\n",
    "        conf = np.zeros((n,n))\n",
    "\n",
    "        for i in range(len(class_uniq)):\n",
    "            for j in range(len(class_uniq)):\n",
    "                conf[i, j] = np.sum((actual_Classes == class_uniq[i]) & (predicted_Classes == class_uniq[j]))\n",
    "\n",
    "    # The rows are actual and columns are predicted\n",
    "    # The first row is positive and second one is of negative class. The same for columns            \n",
    "\n",
    "    # Evaluation Metrics\n",
    "        EM = {}\n",
    "        EM['accuracy'] = round(sum(np.diagonal(conf))/len(data[0]),3)\n",
    "        EM['prec_positive'] = round(conf[0][0]/sum(conf[:,0]),3)\n",
    "        temp_rec = round(conf[0][0]/sum(conf[0,:]),3)\n",
    "        temp_func = lambda temp: 0 if bool(np.isnan(temp))  else temp\n",
    "        EM['rec_positive'] = temp_func(temp_rec)\n",
    "    #     print(EM['rec_positive'])\n",
    "        EM['F1_positive'] = temp_func(round((2*EM['prec_positive']*EM['rec_positive'])/(EM['prec_positive']+EM['rec_positive']),3))\n",
    "        EM['TPR'] = EM['rec_positive']\n",
    "        EM['sensitivity'] = EM['rec_positive']\n",
    "        EM['TNR'] = temp_func(round(conf[1][1]/sum(conf[1,:]),3))\n",
    "        EM['specificity'] = EM['TNR']\n",
    "        EM['FPR'] = temp_func(round(1 - EM['TNR'],3))\n",
    "        return EM, conf\n",
    "\n",
    "    def ROC(data, inc):\n",
    "        t = 0\n",
    "        a = []\n",
    "        while t <= 1:\n",
    "            EM_temp = evaluation_metrics(data, t)[0]\n",
    "            b =[EM_temp['FPR'], EM_temp['TPR']]\n",
    "            a.append(b)\n",
    "            t += inc\n",
    "        return a\n",
    "\n",
    "    def auc(a):\n",
    "\n",
    "    # Using Trapeziodal principle to find the area under the roc curve\n",
    "\n",
    "        area = 0\n",
    "        for i in range(len(a[0])-1):\n",
    "    #         print(i)\n",
    "            area += abs((a[0][i+1] - a[0][i])*(a[1][i+1] + a[1][i]))/2\n",
    "        return round(area,3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    temp = np.loadtxt(fname = os.path.join(file_path, sampletxt), delimiter= '\\n', dtype= str)\n",
    "    data_temp = [i.replace('{','').replace('}', '').split(', ') for i in temp]\n",
    "    data = [[float(i) for i in j] for j in data_temp]\n",
    "\n",
    "    # data2= predicted_class(data, 0.5)\n",
    "    EM, conf = evaluation_metrics(data, 0.5)\n",
    "\n",
    "    a = np.transpose(ROC(data, inc))\n",
    "\n",
    "\n",
    "\n",
    "    print('(Accuracy ' + str(EM['accuracy']) + ')')\n",
    "    print('(Precision ' + str(EM['prec_positive']) + ')')\n",
    "    print('(Recall ' + str(EM['rec_positive']) + ')')\n",
    "    print('(F1 ' + str(EM['F1_positive']) + ')')\n",
    "    print('(TPR ' + str(EM['TPR']) + ')')\n",
    "    print('(FPR ' + str(EM['FPR']) + ')')\n",
    "    print('(Specificity ' + str(EM['specificity']) + ')')\n",
    "    print('(Sensitivity ' + str(EM['sensitivity']) + ')')\n",
    "    print('(AUC ' + str(auc(a)) + ')')\n",
    "\n",
    "file_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Assignements\\Assignment 3'\n",
    "EM_assignment3(file_path, 'Sample2.txt', 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
